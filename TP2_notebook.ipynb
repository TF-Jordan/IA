{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb420a4d-7e3a-4eb5-a9b9-1c76ae4d18fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PHASE I: R√©gression Lin√©aire Avanc√©e et Analyse des Compromis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4fefa6-0ed4-4b1e-b1eb-ffdc4700e0bd",
   "metadata": {},
   "source": [
    "## Tache 1.1 - Charger et Pr√©parer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d648754-6d0f-4caa-b294-ff79a4d92014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import des biblioth√®ques n√©cessaires\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#2. Chargement du dataset Diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "#3. Division en train(80%) et test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#4. Normalisation des donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c32af2-78e0-42ba-88f9-bba5c626d4e8",
   "metadata": {},
   "source": [
    "## Tache 1.2 - R√©gression OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32a2953-d1f6-4843-afe7-115d75aad336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS: RMSE = 54.52, R¬≤ = 0.44\n",
      "Coefficients OLS: [  1.75375799 -11.51180908  25.60712144  16.82887167 -44.44885564\n",
      "  24.64095356   7.67697768  13.1387839   35.16119521   2.35136365]\n"
     ]
    }
   ],
   "source": [
    "# 1. Cr√©er et entra√Æner un mod√®le OLS (Ordinary Least Squares)\n",
    "ols_reg = LinearRegression()\n",
    "ols_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#2. Pr√©dictions sur le test set\n",
    "y_pred_ols = ols_reg.predict(X_test_scaled)\n",
    "\n",
    "#3. Calcul des metriques\n",
    "rmse_ols = np.sqrt(mean_squared_error(y_test, y_pred_ols))\n",
    "r2_ols = r2_score(y_test, y_pred_ols)\n",
    "\n",
    "#4. Affichage des resultats R¬≤ % variance expliqu√©e (0 √† 1, plus haut = mieux)\n",
    "print(f\"OLS: RMSE = {rmse_ols:.2f}, R¬≤ = {r2_ols:.2f}\")\n",
    "print(\"Coefficients OLS:\", ols_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9e42e-5010-4862-8ed6-0a6bfc370142",
   "metadata": {},
   "source": [
    "## Tache 1.3 - TH√âORIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f038a-a1a0-436c-8522-14da0ea379b1",
   "metadata": {},
   "source": [
    "### 1. Biais (Bias)\n",
    "Le biais est une erreur due aux hypoth√®ses simplificatrices du mod√®le\n",
    "   - Mod√®le TROP SIMPLE ‚Üí sous-apprentissage (underfitting)\n",
    "   - Exemple: r√©gression lin√©aire sur donn√©es non-lin√©aires\n",
    "\n",
    "### 2.  VARIANCE:\n",
    "La variance est une erreur due a une forte sensibilit√© du mod√®le aux variations dans les donn√©es d'entra√Ænement\n",
    "   - Mod√®le TROP COMPLEXE ‚Üí sur-apprentissage (overfitting)\n",
    "   - Exemple: polyn√¥me degr√© 50 sur 100 points\n",
    "\n",
    "### 3. RELATION:\n",
    "   Erreur totale = Bias¬≤ + Variance + Bruit irr√©ductible\n",
    "   \n",
    "   Trade-off: R√©duire l'un augmente l'autre\n",
    "   - ‚Üì Bias ‚Üí ‚Üë Complexit√© ‚Üí ‚Üë Variance\n",
    "   - ‚Üì Variance ‚Üí ‚Üì Complexit√© ‚Üí ‚Üë Bias\n",
    "\n",
    "### 4. COMMENT LA R√âGULARISATION MODIFIE CE TRADE-OFF:\n",
    "   \n",
    "   Sans r√©gularisation (OLS):\n",
    "   Loss = MSE = Œ£(y - ≈∑)¬≤\n",
    "   ‚Üí Minimise erreur training ‚Üí Risque overfitting (haute variance)\n",
    "   \n",
    "   Avec r√©gularisation:\n",
    "   Loss = MSE + Œª √ó P√©nalit√©(coefficients)\n",
    "   \n",
    "   Ridge (L2): Loss = MSE + Œª √ó Œ£w·µ¢¬≤\n",
    "   LASSO (L1): Loss = MSE + Œª √ó Œ£|w·µ¢|\n",
    "   \n",
    "   EFFET DE Œª (hyperparam√®tre):\n",
    "   - Œª = 0: Pas de r√©gularisation ‚Üí OLS standard (haute variance possible)\n",
    "   - Œª petit: L√©g√®re contrainte ‚Üí r√©duit variance un peu\n",
    "   - Œª grand: Forte contrainte ‚Üí r√©duit variance beaucoup mais ‚Üë bias\n",
    "   - Œª ‚Üí ‚àû: Tous coefficients ‚Üí 0 ‚Üí mod√®le constant (bias max)\n",
    "\n",
    "### 5. PR√âVENTION DE L'OVERFITTING:\n",
    "   \n",
    "   La r√©gularisation ajoute une CONTRAINTE sur la magnitude des coefficients:\n",
    "   - Emp√™che les coefficients d'exploser\n",
    "   - Force le mod√®le √† √™tre plus SIMPLE\n",
    "   - R√©duit la sensibilit√© au bruit dans les donn√©es\n",
    "   - Am√©liore la G√âN√âRALISATION sur nouvelles donn√©es\n",
    "   \n",
    "   R√©sultat: ‚Üë Bias l√©g√®rement, mais ‚Üì‚Üì Variance significativement\n",
    "   ‚Üí Erreur test globale diminue (meilleure g√©n√©ralisation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7143b-b8b1-4a8f-be29-ccf0ce41a841",
   "metadata": {},
   "source": [
    "## Tache 1.4 - Ridge Regression (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82166d1-46d9-4ebe-b927-70d37429ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (alpha=1.0): RMSE CV=55.92\n",
      "Coefficients Ridge: [  1.80734179 -11.44818951  25.73269892  16.73429974 -34.67195409\n",
      "  17.05307485   3.36991411  11.76426044  31.3783838    2.45813922]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 1. Entrainement du modele Ridge avec alpha=1.0\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. Evaluation par validation crois√©e\n",
    "scores_ridge = cross_val_score(\n",
    "    ridge_reg, X_train_scaled, y_train,\n",
    "    scoring='neg_root_mean_squared_error', cv=5\n",
    ")\n",
    "rmse_cv_ridge = -scores_ridge.mean()\n",
    "\n",
    "\n",
    "# 3. Afficher r√©sultats\n",
    "print(f\"Ridge (alpha=1.0): RMSE CV={rmse_cv_ridge:.2f}\")\n",
    "print(\"Coefficients Ridge:\", ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cb26e-9338-4a70-a68b-03b9b51e4d85",
   "metadata": {},
   "source": [
    "## POURQUOI RIDGE R√âDUIT MAIS NE MET PAS √Ä Z√âRO\n",
    "\n",
    "1. P√âNALIT√â L2 vs L1:\n",
    "   \n",
    "   - Ridge (L2): P√©nalit√© = Œª √ó Œ£w·µ¢¬≤\n",
    "   - LASSO (L1): P√©nalit√© = Œª √ó Œ£|w·µ¢|\n",
    "\n",
    "---\n",
    "2. D√âRIV√âE - C'EST LA CL√â:\n",
    "   \n",
    "   Ridge: ‚àÇ(w·µ¢¬≤)/‚àÇw·µ¢ = 2w·µ¢\n",
    "   ‚Üí Gradient proportionnel au coefficient\n",
    "   ‚Üí Quand w·µ¢ ‚Üí 0, gradient ‚Üí 0 aussi\n",
    "   ‚Üí R√©duction ASYMPTOTIQUE vers 0 mais jamais 0 exactement\n",
    "   \n",
    "   LASSO: ‚àÇ|w·µ¢|/‚àÇw·µ¢ = sign(w·µ¢) = {+1 si w·µ¢>0, -1 si w·µ¢<0}\n",
    "   ‚Üí Gradient CONSTANT quelle que soit la valeur de w·µ¢\n",
    "   ‚Üí Pousse directement √† 0 (seuillage dur)\n",
    "---\n",
    "3. G√âOM√âTRIE - FORME DES CONTRAINTES:\n",
    "   \n",
    "   Ridge: Contrainte = boule (cercle en 2D)\n",
    "   ‚Üí Bords lisses et courbes\n",
    "   ‚Üí Solution optimale peut √™tre n'importe o√π sur le bord\n",
    "   ‚Üí Probabilit√© faible de toucher les axes (w=0)\n",
    "   \n",
    "   LASSO: Contrainte = losange (diamant en 2D)\n",
    "   ‚Üí COINS sur les axes\n",
    "   ‚Üí Solution optimale tend vers les coins\n",
    "   ‚Üí Probabilit√© √âLEV√âE que w = 0 exactement\n",
    "---\n",
    "4. COMPORTEMENT PRATIQUE:\n",
    "   \n",
    "   Ridge avec Œª croissant:\n",
    "   w = [100, 50, 10] ‚Üí [50, 25, 5] ‚Üí [10, 5, 1] ‚Üí [0.01, 0.005, 0.001]\n",
    "   ‚Üí JAMAIS exactement 0, mais tr√®s proche\n",
    "   \n",
    "   LASSO avec Œª croissant:\n",
    "   w = [100, 50, 10] ‚Üí [80, 30, 0] ‚Üí [50, 0, 0] ‚Üí [0, 0, 0]\n",
    "   ‚Üí Met √† 0 progressivement (s√©lection de features)\n",
    "---\n",
    "5. AVANTAGE RIDGE:\n",
    "   - Garde toutes les features (utile si toutes importantes)\n",
    "   - Stable num√©riquement\n",
    "   - Solution unique m√™me si p > n\n",
    "   \n",
    "   AVANTAGE LASSO:\n",
    "   - S√©lection automatique de features\n",
    "   - Mod√®le sparse et interpr√©table\n",
    "   - R√©duit la dimensionnalit√©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdcbe3-de53-42b2-9660-ba9b55cadcf8",
   "metadata": {},
   "source": [
    "## 1.3 Sparsity et Optimisation des Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd0102-102f-4823-affe-1997231a99f4",
   "metadata": {},
   "source": [
    "## TASK 1.5 - LASSO et Concept de Sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24e6e36-6f3d-4376-bab2-6eb6ad42b3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients LASSO: [  1.73045056 -11.31635911  25.82462699  16.64425156 -29.35841191\n",
      "  13.27584411   0.5479479   10.23616805  29.63282611   2.39347521]\n",
      "Nombre de coefficients = 0: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 1. Entra√Æner LASSO avec alpha initial\n",
    "lasso_reg = Lasso(alpha=0.1, max_iter=10000)\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. Afficher coefficients\n",
    "print(\"Coefficients LASSO:\", lasso_reg.coef_)\n",
    "print(f\"Nombre de coefficients = 0: {np.sum(lasso_reg.coef_ == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a77826-8079-40ae-ac83-904dc693660d",
   "metadata": {},
   "source": [
    "## Sparsity (Parcimonie)\n",
    "\n",
    "### D√©finition\n",
    "La sparsity d√©signe un mod√®le dont **beaucoup de coefficients sont exactement nuls**.\n",
    "\n",
    "\\[\n",
    "w = (w_1,\\dots,w_p), \\quad \\text{avec } w_j = 0 \\text{ pour beaucoup de } j\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Int√©r√™t (interpr√©tabilit√©)\n",
    "- Moins de variables actives\n",
    "- Mod√®le plus lisible\n",
    "- Identification claire des features utiles\n",
    "\n",
    "---\n",
    "\n",
    "### Pourquoi L1 cr√©e des z√©ros et pas L2 ?\n",
    "\n",
    "### P√©nalit√©s\n",
    "- **L1 (LASSO)** :  \n",
    "\\[\n",
    "\\lambda \\sum_j |w_j|\n",
    "\\]\n",
    "\n",
    "- **L2 (Ridge)** :  \n",
    "\\[\n",
    "\\lambda \\sum_j w_j^2\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Diff√©rentiabilit√©\n",
    "- **L2** : d√©rivable partout ‚Üí coefficients r√©duits mais jamais nuls\n",
    "- **L1** : non diff√©rentiable en 0 ‚Üí solution possible exactement en 0\n",
    "\n",
    "---\n",
    "\n",
    "### G√©om√©trie\n",
    "- **L2** : contrainte circulaire ‚Üí contact rarement sur les axes\n",
    "- **L1** : contrainte en losange ‚Üí coins sur les axes ‚Üí coefficients nuls\n",
    "\n",
    "---\n",
    "\n",
    "### Soft Thresholding (LASSO)\n",
    "\\[\n",
    "\\hat w = \\text{sign}(z)\\max(|z| - \\lambda, 0)\n",
    "\\]\n",
    "\n",
    "- Si \\(|z| \\le \\lambda\\) ‚Üí \\(w = 0\\)\n",
    "\n",
    "---\n",
    "\n",
    "## Avantages du LASSO\n",
    "- S√©lection automatique de variables\n",
    "- R√©duction de dimension\n",
    "- Efficace quand \\(p > n\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc795abb-87ca-4012-85c4-ebe3dbae4ae4",
   "metadata": {},
   "source": [
    "##  Tache 1.6 - Optimisation avec Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b82d12-0b54-406c-b41d-64f885c83000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha optimal pour Ridge: 55.9081\n",
      "Alpha optimal pour LASSO: 0.1417\n",
      "\n",
      "Coefficients LASSO finaux:\n",
      "  Feature 0: 1.6966 \n",
      "  Feature 1: -11.2072 \n",
      "  Feature 2: 25.9031 \n",
      "  Feature 3: 16.5961 \n",
      "  Feature 4: -27.1653 \n",
      "  Feature 5: 11.3606 \n",
      "  Feature 6: -0.0000 ‚Üí Z√âRO\n",
      "  Feature 7: 10.3508 \n",
      "  Feature 8: 28.7237 \n",
      "  Feature 9: 2.3938 \n",
      "\n",
      "Features √©limin√©es: 1/10\n",
      "\n",
      "üìä COMPARAISON FINALE:\n",
      "Mod√®le         RMSE       R¬≤   Features\n",
      "----------------------------------------\n",
      "OLS           54.52   0.4389         10\n",
      "Ridge         53.88   0.4521         10\n",
      "LASSO         54.23   0.4449          9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "\n",
    "# 1. Ridge Tuning - teste 100 valeurs d'alpha\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-3, 2, 100), cv=10)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "alpha_ridge_optimal = ridge_cv.alpha_\n",
    "\n",
    "print(f\"Alpha optimal pour Ridge: {alpha_ridge_optimal:.4f}\")\n",
    "\n",
    "# 2. LASSO Tuning - teste 100 valeurs d'alpha\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-3, 0, 100), cv=10, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "alpha_lasso_optimal = lasso_cv.alpha_\n",
    "\n",
    "print(f\"Alpha optimal pour LASSO: {alpha_lasso_optimal:.4f}\")\n",
    "\n",
    "# 3. Mod√®le LASSO final avec alpha optimal\n",
    "lasso_final = Lasso(alpha=alpha_lasso_optimal, max_iter=10000)\n",
    "lasso_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Analyse de la Sparsity\n",
    "print(\"\\nCoefficients LASSO finaux:\")\n",
    "for i, coef in enumerate(lasso_final.coef_):\n",
    "    status = \"‚Üí Z√âRO\" if coef == 0 else \"\"\n",
    "    print(f\"  Feature {i}: {coef:.4f} {status}\")\n",
    "\n",
    "zero_count = np.sum(lasso_final.coef_ == 0)\n",
    "print(f\"\\nFeatures √©limin√©es: {zero_count}/{len(lasso_final.coef_)}\")\n",
    "\n",
    "# 5. √âvaluation finale des 3 mod√®les\n",
    "y_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso_final.predict(X_test_scaled)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# 6. Tableau comparatif\n",
    "print(\"\\nüìä COMPARAISON FINALE:\")\n",
    "print(f\"{'Mod√®le':<10} {'RMSE':>8} {'R¬≤':>8} {'Features':>10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'OLS':<10} {rmse_ols:>8.2f} {r2_ols:>8.4f} {10:>10}\")\n",
    "print(f\"{'Ridge':<10} {rmse_ridge:>8.2f} {r2_ridge:>8.4f} {10:>10}\")\n",
    "print(f\"{'LASSO':<10} {rmse_lasso:>8.2f} {r2_lasso:>8.4f} {10-zero_count:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f07ca-705f-4085-9b37-33632aecbdf5",
   "metadata": {},
   "source": [
    "### CE QU'ON OBSERVE:\n",
    "\n",
    "- Ridge garde toutes les features mais r√©duit leurs coefficients\n",
    "- LASSO √©limine certaines features (coefficients = 0)\n",
    "- Comparaison des performances (RMSE, R¬≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dbf8b-c710-4377-bebc-3d7c4885bc4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PHASE II: MLOps - Du Mod√®le √† la Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ab128-247c-44e7-bbe8-2bf82c19d738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Packaging du Mod√®le et Gestion des D√©pendances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc2fe4a-e961-467b-bd57-072995115d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Scaler sauvegard√©: scaler.pkl\n",
      "‚úì Mod√®le LASSO sauvegard√©: lasso_model.pkl\n",
      "\n",
      "[Test Pr√©diction]\n",
      "  Pr√©dite: 150.95\n",
      "  R√©elle: 219.00\n",
      "  Erreur: 68.05\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Sauvegarder le meilleur mod√®le (LASSO) et le scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(lasso_final, 'lasso_model.pkl')\n",
    "\n",
    "print(\"‚úì Scaler sauvegard√©: scaler.pkl\")\n",
    "print(\"‚úì Mod√®le LASSO sauvegard√©: lasso_model.pkl\")\n",
    "\n",
    "# 2. Fonction de pr√©diction en production\n",
    "def predict_new_data(raw_data, model_path='lasso_model.pkl', \n",
    "                     scaler_path='scaler.pkl'):\n",
    "\n",
    "    try:\n",
    "        # Charger les objets sauvegard√©s\n",
    "        loaded_scaler = joblib.load(scaler_path)\n",
    "        loaded_model = joblib.load(model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Erreur: Fichiers introuvables\")\n",
    "        return None\n",
    "    \n",
    "    # √âtape 1: Transformer avec le M√äME scaler (CRUCIAL!)\n",
    "    scaled_data = loaded_scaler.transform([raw_data])\n",
    "    \n",
    "    # √âtape 2: Pr√©diction\n",
    "    prediction = loaded_model.predict(scaled_data)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 3. Test de la fonction\n",
    "example_input = X_test[0]\n",
    "predicted = predict_new_data(example_input)\n",
    "actual = y_test[0]\n",
    "\n",
    "print(f\"\\n[Test Pr√©diction]\")\n",
    "print(f\"  Pr√©dite: {predicted:.2f}\")\n",
    "print(f\"  R√©elle: {actual:.2f}\")\n",
    "print(f\"  Erreur: {abs(predicted - actual):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14bb361-bbfa-43c8-9f58-42d344fb7d3b",
   "metadata": {},
   "source": [
    "## Pourquoi sauvegarder le scaler ?\n",
    "\n",
    "### 1. Coh√©rence des transformations\n",
    "Un mod√®le apprend **sur des donn√©es transform√©es**, pas sur les donn√©es brutes.  \n",
    "Si le scaler utilis√© en production n‚Äôest pas **exactement** celui du training, alors les entr√©es du mod√®le ne sont plus dans le m√™me espace.  \n",
    "M√™me mod√®le, m√™mes features, **mais monde math√©matique diff√©rent** ‚Üí pr√©dictions incoh√©rentes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Exemple concret\n",
    "**Training**\n",
    "- Feature : BMI  \n",
    "- Moyenne = 25  \n",
    "- √âcart-type = 5  \n",
    "\n",
    "Standardisation :\n",
    "\\[\n",
    "z = \\frac{BMI - 25}{5}\n",
    "\\]\n",
    "\n",
    "**Production**\n",
    "- Nouvelle donn√©e : BMI = 30  \n",
    "\n",
    "Avec le bon scaler :\n",
    "\\[\n",
    "z = \\frac{30 - 25}{5} = 1\n",
    "\\]\n",
    "\n",
    "Sans le scaler sauvegard√© (ex: moyenne=27, std=3 recalcul√©s) :\n",
    "\\[\n",
    "z = \\frac{30 - 27}{3} = 1\n",
    "\\]\n",
    "\n",
    "M√™me valeur brute, **repr√©sentation diff√©rente**, donc le mod√®le voit **un autre individu**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Risques\n",
    "- Pr√©dictions fausses ou instables  \n",
    "- D√©gradation silencieuse des performances  \n",
    "- Impossibilit√© de reproduire les r√©sultats  \n",
    "- Mod√®le inutilisable en production  \n",
    "- Debug infernal sans cause √©vidente  \n",
    "\n",
    "En clair : le mod√®le fonctionne, mais raconte n‚Äôimporte quoi.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. R√®gle d‚Äôor (MLOps)\n",
    "**Scaler + mod√®le = un seul artefact.**  \n",
    "On **fit** le scaler sur le training, on **le sauvegarde**, et on le **r√©utilise tel quel** en production.  \n",
    "Jamais de refit sur les donn√©es de production. Jamais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36533dcb-5826-457a-8b33-6ab97515e393",
   "metadata": {},
   "source": [
    "## 2.2 MLflow pour la Gestion du Cycle de Vie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f15a1f-338f-4616-8515-3c6dd05b18f4",
   "metadata": {},
   "source": [
    "### Tache 2.3 - Tracking avec MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f24591a-037c-4fec-ac22-c27cccfff01b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/03 15:13:32 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/03 15:13:32 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/03 15:13:32 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/03 15:13:32 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2026/01/03 15:13:33 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2026/01/03 15:13:34 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2026/01/03 15:13:35 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/03 15:13:35 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/03 15:13:35 INFO mlflow.tracking.fluent: Experiment with name 'TP_Advanced_Regression' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Run 'OLS_Baseline' enregistr√© dans MLflow\n",
      "‚úì Run 'Ridge_Optimal' enregistr√© dans MLflow\n",
      "‚úì Run 'LASSO_Optimal' enregistr√© dans MLflow\n",
      "\n",
      "‚úì Tous les mod√®les enregistr√©s dans MLflow\n",
      "Pour visualiser: mlflow ui\n",
      "Puis ouvrir: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# 1. Configurer l'exp√©rience\n",
    "mlflow.set_experiment(\"TP_Advanced_Regression\")\n",
    "\n",
    "def log_model_results(model_name, params, metrics, model_object):\n",
    "    \"\"\"\n",
    "    Log les r√©sultats d'un mod√®le dans MLflow.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nom du run (ex: \"OLS\", \"Ridge\", \"LASSO\")\n",
    "        params: Dictionnaire des hyperparam√®tres\n",
    "        metrics: Dictionnaire des m√©triques (RMSE, R¬≤)\n",
    "        model_object: Objet mod√®le sklearn\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log hyperparam√®tres\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log m√©triques\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log mod√®le comme artifact\n",
    "        mlflow.sklearn.log_model(model_object, name=\"model\")\n",
    "        \n",
    "        print(f\"‚úì Run '{model_name}' enregistr√© dans MLflow\")\n",
    "\n",
    "# 2. Logger OLS\n",
    "ols_params = {\"model_type\": \"OLS\", \"regularization\": \"none\"}\n",
    "ols_metrics = {\"test_rmse\": rmse_ols, \"test_r2\": r2_ols}\n",
    "log_model_results(\"OLS_Baseline\", ols_params, ols_metrics, ols_reg)\n",
    "\n",
    "# 3. Logger Ridge\n",
    "ridge_params = {\n",
    "    \"alpha\": alpha_ridge_optimal,\n",
    "    \"solver\": \"auto\",\n",
    "    \"penalty\": \"l2\"\n",
    "}\n",
    "ridge_metrics = {\"test_rmse\": rmse_ridge, \"test_r2\": r2_ridge}\n",
    "log_model_results(\"Ridge_Optimal\", ridge_params, ridge_metrics, ridge_cv)\n",
    "\n",
    "# 4. Logger LASSO\n",
    "lasso_params = {\n",
    "    \"alpha\": alpha_lasso_optimal,\n",
    "    \"solver\": \"coordinate_descent\",\n",
    "    \"penalty\": \"l1\"\n",
    "}\n",
    "lasso_metrics = {\n",
    "    \"test_rmse\": rmse_lasso,\n",
    "    \"test_r2\": r2_lasso,\n",
    "    \"num_zero_coefs\": zero_count,\n",
    "    \"sparsity_ratio\": zero_count / len(lasso_final.coef_)\n",
    "}\n",
    "log_model_results(\"LASSO_Optimal\", lasso_params, lasso_metrics, lasso_final)\n",
    "\n",
    "print(\"\\n‚úì Tous les mod√®les enregistr√©s dans MLflow\")\n",
    "print(\"Pour visualiser: mlflow ui\")\n",
    "print(\"Puis ouvrir: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eabd5-97cd-47c7-bd6f-f25773ea2983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explication du Dockerfile\n",
    "\n",
    "### FROM\n",
    "- **R√¥le** : D√©finit l‚Äôimage de base sur laquelle le conteneur est construit.  \n",
    "  Elle fournit l‚ÄôOS minimal, le runtime et les outils de base.\n",
    "- **Pourquoi `python:3.9-slim` ?**  \n",
    "  - Python d√©j√† install√©  \n",
    "  - Image l√©g√®re (moins de surface d‚Äôattaque, build plus rapide)  \n",
    "  - Suffisant pour ex√©cuter une app Python sans d√©pendances inutiles  \n",
    "\n",
    "---\n",
    "\n",
    "### WORKDIR\n",
    "- **R√¥le** : D√©finit le r√©pertoire de travail √† l‚Äôint√©rieur du conteneur.  \n",
    "  Toutes les commandes suivantes (`COPY`, `RUN`, `CMD`) s‚Äôex√©cutent depuis ce dossier.  \n",
    "  √âvite les chemins absolus et le d√©sordre.\n",
    "\n",
    "---\n",
    "\n",
    "### COPY\n",
    "- **R√¥le** : Copie des fichiers du syst√®me h√¥te vers le syst√®me de fichiers du conteneur.\n",
    "- **Diff√©rence entre `COPY requirements.txt` et `COPY app.py` ?**  \n",
    "  - `requirements.txt` : utilis√© pour installer les d√©pendances  \n",
    "  - `app.py` : contient le code applicatif  \n",
    "  S√©parer les deux permet √† Docker de **mettre en cache l‚Äôinstallation des d√©pendances** si le code change mais pas les libs.\n",
    "\n",
    "---\n",
    "\n",
    "### RUN\n",
    "- **R√¥le** : Ex√©cute une commande **au moment du build** de l‚Äôimage.  \n",
    "  Typiquement utilis√© pour installer des d√©pendances ou configurer l‚Äôenvironnement.\n",
    "- **Quand est ex√©cut√©e ?**  \n",
    "  Une seule fois, lors du `docker build`.  \n",
    "  Le r√©sultat est fig√© dans l‚Äôimage finale.\n",
    "\n",
    "---\n",
    "\n",
    "### EXPOSE\n",
    "- **R√¥le** : Documente le port sur lequel l‚Äôapplication √©coute √† l‚Äôint√©rieur du conteneur.\n",
    "- **Est-ce que √ßa ouvre vraiment le port ?**  \n",
    "  Non.  \n",
    "  √áa n‚Äôouvre rien du tout.  \n",
    "  Le port est r√©ellement expos√© uniquement avec `-p` ou `--publish` au `docker run`.\n",
    "\n",
    "---\n",
    "\n",
    "### ENV\n",
    "- **R√¥le** : D√©finit des variables d‚Äôenvironnement disponibles dans le conteneur.  \n",
    "  Utilis√©es pour la configuration (mode debug, ports, chemins, cl√©s, etc.).  \n",
    "  √âvite le hard-coding dans le code.\n",
    "\n",
    "---\n",
    "\n",
    "### CMD\n",
    "- **R√¥le** : D√©finit la commande **par d√©faut** ex√©cut√©e au d√©marrage du conteneur.\n",
    "- **Diff√©rence entre `CMD` et `RUN` ?**  \n",
    "  - `RUN` ‚Üí ex√©cut√© au build, une seule fois  \n",
    "  - `CMD` ‚Üí ex√©cut√© au runtime, √† chaque lancement du conteneur  \n",
    "\n",
    "Sans `CMD`, ton conteneur d√©marre‚Ä¶ et s‚Äôarr√™te imm√©diatement. Classe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cfd91-4d36-4d4b-93ad-afac91e20ed1",
   "metadata": {},
   "source": [
    "## Synth√®se MLOps: Git + MLflow + Docker\n",
    "\n",
    "### 1. GIT ‚Äì Version Control\n",
    "**R√¥le:**\n",
    "- Gestion des versions du code\n",
    "- Tra√ßabilit√© des changements\n",
    "- Collaboration propre et reproductible\n",
    "\n",
    "**Pour le ML:**\n",
    "- Versionnement du code d‚Äôentra√Ænement\n",
    "- Suivi des features, scripts, configs\n",
    "- Lien clair entre code, donn√©es et mod√®le entra√Æn√©\n",
    "\n",
    "**CI/CD:**\n",
    "- `git push` d√©clenche automatiquement les pipelines (tests, training, build, deploy)\n",
    "- Base de toute automatisation fiable\n",
    "\n",
    "---\n",
    "\n",
    "### 2. MLFLOW ‚Äì Lifecycle Management\n",
    "\n",
    "**4 Composantes:**\n",
    "- **Tracking** : log des param√®tres, m√©triques, artefacts ‚Üí reproductibilit√©\n",
    "- **Projects** : standardisation des runs (environnement, entry point)\n",
    "- **Models** : format unifi√© pour charger/servir les mod√®les\n",
    "- **Registry** : versioning, staging, production, rollback\n",
    "\n",
    "**CI/CD:**\n",
    "- Enregistrement automatique des mod√®les apr√®s training\n",
    "- Promotion contr√¥l√©e (Staging ‚Üí Production)\n",
    "- Int√©gration directe dans les pipelines de d√©ploiement\n",
    "\n",
    "---\n",
    "\n",
    "### 3. DOCKER ‚Äì Containerisation\n",
    "\n",
    "**R√¥le:**\n",
    "- Emballer code + d√©pendances + environnement\n",
    "- Garantir ‚Äú√ßa marche chez moi = √ßa marche partout‚Äù\n",
    "\n",
    "**Avantages ML:**\n",
    "- Environnement identique training / inference\n",
    "- Isolation des d√©pendances (librairies, CUDA, versions)\n",
    "- D√©ploiement reproductible\n",
    "\n",
    "**CI/CD:**\n",
    "- Build d‚Äôimages automatis√©\n",
    "- D√©ploiement rapide sur serveur, cloud ou Kubernetes\n",
    "- Rollback simple par version d‚Äôimage\n",
    "\n",
    "---\n",
    "\n",
    "### 4. PIPELINE CI/CD COMPLET\n",
    "\n",
    "**Workflow typique:**\n",
    "1. **Data Scientist** : `git push`\n",
    "2. **CI** :\n",
    "   - Tests du code\n",
    "   - Entra√Ænement du mod√®le\n",
    "   - Logging MLflow (metrics, mod√®le)\n",
    "   - Build image Docker\n",
    "3. **CD** :\n",
    "   - R√©cup√©ration du mod√®le valid√© (Registry)\n",
    "   - D√©ploiement automatique en production\n",
    "4. **Monitoring** :\n",
    "   - Surveillance des donn√©es, performances, d√©rives\n",
    "   - Alertes et retraining si n√©cessaire\n",
    "\n",
    "---\n",
    "\n",
    "### 5. EXEMPLE CONCRET\n",
    "\n",
    "**Sans MLOps:**\n",
    "- Mod√®le entra√Æn√© localement\n",
    "- R√©sultats non reproductibles\n",
    "- D√©ploiement manuel\n",
    "- Bugs et d√©rives non d√©tect√©s\n",
    "\n",
    "**Avec MLOps:**\n",
    "- Code versionn√© (Git)\n",
    "- Mod√®le tra√ßable et versionn√© (MLflow)\n",
    "- D√©ploiement stable et reproductible (Docker)\n",
    "- Monitoring continu et rollback possible\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring Post-Production\n",
    "\n",
    "### 1. DATA DRIFT (D√©rive des Donn√©es)\n",
    "\n",
    "**D√©finition:**\n",
    "Changement de la distribution des donn√©es d‚Äôentr√©e entre le training et la production.\n",
    "\n",
    "**Causes:**\n",
    "- √âvolution du comportement utilisateur\n",
    "- Changements saisonniers\n",
    "- Capteurs d√©fectueux\n",
    "- Nouvelles sources de donn√©es\n",
    "\n",
    "**D√©tection:**\n",
    "- **Tests statistiques** : KS-test, Chi-square, PSI\n",
    "- **M√©triques** : variation de moyenne, variance, histogrammes\n",
    "\n",
    "**Exemple concret (Diabetes):**\n",
    "La distribution de la glyc√©mie moyenne augmente fortement apr√®s plusieurs mois ‚Üí le mod√®le re√ßoit des profils jamais vus au training.\n",
    "\n",
    "**Actions:**\n",
    "- Alerte\n",
    "- Analyse des features impact√©es\n",
    "- Retraining avec donn√©es r√©centes\n",
    "- Validation avant red√©ploiement\n",
    "\n",
    "---\n",
    "\n",
    "### 2. CONCEPT DRIFT (D√©rive du Concept)\n",
    "\n",
    "**D√©finition:**\n",
    "La relation entre les features et la cible change, m√™me si les donn√©es semblent similaires.\n",
    "\n",
    "**Causes:**\n",
    "- Changements m√©dicaux (nouveaux traitements)\n",
    "- √âvolution des comportements\n",
    "- Politiques ou r√®gles m√©tier modifi√©es\n",
    "\n",
    "**D√©tection:**\n",
    "- Baisse progressive des performances\n",
    "- Erreurs syst√©matiques sur certains segments\n",
    "\n",
    "**M√©triques:**\n",
    "- Accuracy, F1, AUC, calibration\n",
    "- Erreur par sous-population\n",
    "\n",
    "---\n",
    "\n",
    "### AUTRES M√âTRIQUES IMPORTANTES\n",
    "- **Latence / Throughput** : temps de r√©ponse et capacit√© du syst√®me\n",
    "- **Resource Usage** : CPU, RAM, GPU, co√ªts cloud\n",
    "- **Business Metrics** : impact r√©el (revenu, taux d‚Äôerreur m√©tier, satisfaction)\n",
    "\n",
    "---\n",
    "\n",
    "### DASHBOARD ID√âAL\n",
    "- Performances du mod√®le dans le temps\n",
    "- Data drift par feature\n",
    "- Concept drift via m√©triques de sortie\n",
    "- Latence et ressources syst√®me\n",
    "- Alertes automatiques et historique des versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea01b76-74df-4d97-9da1-f730f625f198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
